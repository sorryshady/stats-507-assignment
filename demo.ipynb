{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe My Environment - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the **Describe My Environment** system.\n",
    "\n",
    "**Features:**\n",
    "1. **Reflex Loop**: Low-latency object tracking and hazard detection (YOLO11).\n",
    "2. **Cognitive Loop**: Scene understanding (BLIP) and narration (Llama 3.2).\n",
    "3. **Audio Feedback**: Text-to-speech narration and hazard warnings.\n",
    "\n",
    "You can choose to run a **Static Test** (single image) or a **Live Camera Demo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.reflex_loop.tracker import YOLOTracker\n",
    "from src.reflex_loop.safety import SafetyMonitor\n",
    "from src.cognitive_loop.scene_composer import SceneComposer\n",
    "from src.cognitive_loop.narrator import LLMNarrator\n",
    "from src.cognitive_loop.history import HistoryBuffer\n",
    "from src.hardware.audio import AudioHandler\n",
    "from src.config import GLOBAL_WARNING_COOLDOWN\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_image_test():\n",
    "    print(\"\\n--- Running Static Image Test ---\")\n",
    "    \n",
    "    # Initialize Components\n",
    "    print(\"‚è≥ Initializing components...\")\n",
    "    tracker = YOLOTracker(model_path=\"yolo11n.pt\")\n",
    "    scene_composer = SceneComposer()\n",
    "    narrator = LLMNarrator()\n",
    "    audio_handler = AudioHandler()\n",
    "    \n",
    "    # Load Image\n",
    "    image_path = os.path.join(\"test_images\", \"test_image_0.jpg\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"‚ùå Image not found: {image_path}\")\n",
    "        return\n",
    "        \n",
    "    frame = cv2.imread(image_path)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 1. Tracking\n",
    "    detections = tracker.detect(frame)\n",
    "    print(f\"‚úÖ Detected {len(detections)} objects\")\n",
    "    \n",
    "    # Visualize\n",
    "    annotated_frame = frame_rgb.copy()\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det.box\n",
    "        label = f\"{det.class_name} {det.confidence:.2f}\"\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated_frame)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Detections\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Scene Description\n",
    "    caption = scene_composer.generate_scene_description(frame)\n",
    "    print(f\"\\nüìù Scene: {caption}\")\n",
    "    \n",
    "    # 3. Narration\n",
    "    # Prepare strings for compose_prompt\n",
    "    object_descriptions = [\n",
    "        f\"{d.class_name} (conf: {d.confidence:.2f})\" for d in detections\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nü§ñ Generating Narration...\")\n",
    "    \n",
    "    # Use compose_prompt to create the string prompt for generate_narration\n",
    "    prompt = narrator.compose_prompt(caption, object_descriptions)\n",
    "    narration = narrator.generate_narration(prompt)\n",
    "    \n",
    "    if narration:\n",
    "        print(f\"\\nüó£Ô∏è FINAL NARRATION: {narration}\")\n",
    "        audio_handler.speak_text(narration)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not generate narration (check Ollama)\")\n",
    "        \n",
    "    # Allow time for audio to play before cleanup\n",
    "    time.sleep(5)\n",
    "    audio_handler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camera_demo():\n",
    "    print(\"\\n--- Running Live Camera Demo ---\")\n",
    "    print(\"‚è≥ Initializing pipeline components...\")\n",
    "    \n",
    "    tracker = YOLOTracker(model_path=\"yolo11n.pt\")\n",
    "    scene_composer = SceneComposer()\n",
    "    narrator = LLMNarrator()\n",
    "    safety_monitor = SafetyMonitor()\n",
    "    history_buffer = HistoryBuffer()\n",
    "    audio_handler = AudioHandler()\n",
    "    \n",
    "    if not narrator.check_connection():\n",
    "        print(\"‚ö†Ô∏è Ollama is not running. Narration will be disabled.\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Camera started.\")\n",
    "    print(\"Commands:\")\n",
    "    print(\" [Space] - Generate scene narration\")\n",
    "    print(\" [q]     - Quit\")\n",
    "\n",
    "    frame_count = 0\n",
    "    last_hazard_time = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            timestamp = time.time()\n",
    "            frame_count += 1\n",
    "            \n",
    "            # 1. Reflex Loop: Tracking\n",
    "            detections, annotated_frame = tracker.track(frame, frame_count, timestamp, return_annotated=True)\n",
    "            \n",
    "            # Update History\n",
    "            for det in detections:\n",
    "                track_id = det.track_id if det.track_id is not None else -1\n",
    "                history_buffer.add_detection(track_id, det)\n",
    "            \n",
    "            history_buffer.cleanup_stale_objects(frame_count)\n",
    "            \n",
    "            # 2. Hazard Detection\n",
    "            hazards = safety_monitor.check_hazards(detections, history_buffer)\n",
    "            if safety_monitor.should_warn(hazards):\n",
    "                current_time = time.time()\n",
    "                # Add cooldown to prevent spamming the audio queue\n",
    "                if current_time - last_hazard_time > GLOBAL_WARNING_COOLDOWN:\n",
    "                    warning = safety_monitor.get_warning_message(hazards)\n",
    "                    print(f\"‚ö†Ô∏è HAZARD: {warning}\")\n",
    "                    audio_handler.play_beep()\n",
    "                    audio_handler.speak_text(warning, priority=True)\n",
    "                    last_hazard_time = current_time\n",
    "                \n",
    "                # Visual warning always shows\n",
    "                cv2.putText(annotated_frame, \"HAZARD DETECTED\", (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "            # Show Frame\n",
    "            cv2.imshow(\"Describe My Environment - Demo\", annotated_frame)\n",
    "            \n",
    "            # Input Handling\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == 32:  # Spacebar for narration\n",
    "                print(\"\\nüì∏ Creating narration...\")\n",
    "                \n",
    "                # Run narration logic\n",
    "                caption = scene_composer.generate_scene_description(frame)\n",
    "                print(f\"   üìù BLIP Caption: {caption}\")\n",
    "                \n",
    "                # Format objects for the prompt\n",
    "                object_descriptions = [\n",
    "                    f\"{d.class_name} at box {d.box}\" for d in detections\n",
    "                ]\n",
    "                if hazards:\n",
    "                    hazard_descriptions = [str(h.reason) for h in hazards]\n",
    "                    object_descriptions.append(f\"Hazards: {hazard_descriptions}\")\n",
    "                \n",
    "                # Compose the single string prompt\n",
    "                prompt = narrator.compose_prompt(caption, object_descriptions)\n",
    "                print(f\"   üì§ Sending Prompt to Llama: \\n{'-'*20}\\n{prompt}\\n{'-'*20}\")\n",
    "                \n",
    "                # Generate narration\n",
    "                print(\"ü§î Thinking...\")\n",
    "                narration = narrator.generate_narration(prompt)\n",
    "                \n",
    "                if narration:\n",
    "                    print(f\"üó£Ô∏è {narration}\")\n",
    "                    audio_handler.speak_text(narration)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        audio_handler.stop()\n",
    "        print(\"üõë Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Mode:\n",
      "1. Static Test Image\n",
      "2. Live Camera Demo\n",
      "\n",
      "--- Running Live Camera Demo ---\n",
      "‚è≥ Initializing pipeline components...\n",
      "‚úÖ Camera started.\n",
      "Commands:\n",
      " [Space] - Generate scene narration\n",
      " [q]     - Quit\n",
      "\n",
      "üì∏ Creating narration...\n",
      "   üìù BLIP Caption: a man in a black shirt and headphones\n",
      "   üì§ Sending Prompt to Llama: \n",
      "--------------------\n",
      "SYSTEM: You are a helpful assistant for a blind user. Be concise and direct. Only describe what is certainly present. Do not ask questions.\n",
      "If the context mentions a \"mirror\" or \"reflection\" and it seems to be describing the user themselves (e.g., \"standing in front of a mirror\"), assume it is a hallucination caused by the camera feed and describe it as the person being present or facing the camera.\n",
      "\n",
      "USER:\n",
      "Context: \"a man in a black shirt and headphones\"\n",
      "Entities (detected movement):\n",
      "- person at box (627, 308, 1575, 1076)\n",
      "\n",
      "TASK: Synthesize the context and entities into one natural sentence.\n",
      "IMPORTANT RULES:\n",
      "1. The entities likely correspond to the subjects in the context (e.g., a \"person\" entity is likely the same as \"a man\" or \"a woman\" in the context). Do not treat them as separate people unless clearly distinct.\n",
      "2. If the context mentions a person holding an object, and that object also appears in the entities list, DO NOT describe the object as moving independently. It moves with the person.\n",
      "3. Small handheld objects moving in the same direction as a person are almost certainly held items, not independent threats.\n",
      "4. Prioritize safety information about truly independent moving objects (vehicles, other people, animals).\n",
      "5. Ignore any coordinates or bounding box numbers (e.g., \"box (100, 200, ...)\") mentioned in the entities list. They are technical data. If an entity is described as \"at box\", simply treat it as \"present\" or \"in front of you\". Do NOT say \"at a box\", \"near a box\", or \"at the location\".\n",
      "--------------------\n",
      "ü§î Thinking...\n",
      "üó£Ô∏è A man in a black shirt and headphones is present, facing the camera.\n",
      "\n",
      "üì∏ Creating narration...\n",
      "   üìù BLIP Caption: a man in a room brushing his teeth\n",
      "   üì§ Sending Prompt to Llama: \n",
      "--------------------\n",
      "SYSTEM: You are a helpful assistant for a blind user. Be concise and direct. Only describe what is certainly present. Do not ask questions.\n",
      "If the context mentions a \"mirror\" or \"reflection\" and it seems to be describing the user themselves (e.g., \"standing in front of a mirror\"), assume it is a hallucination caused by the camera feed and describe it as the person being present or facing the camera.\n",
      "\n",
      "USER:\n",
      "Context: \"a man in a room brushing his teeth\"\n",
      "Entities (detected movement):\n",
      "- person at box (726, 444, 1502, 1075)\n",
      "\n",
      "TASK: Synthesize the context and entities into one natural sentence.\n",
      "IMPORTANT RULES:\n",
      "1. The entities likely correspond to the subjects in the context (e.g., a \"person\" entity is likely the same as \"a man\" or \"a woman\" in the context). Do not treat them as separate people unless clearly distinct.\n",
      "2. If the context mentions a person holding an object, and that object also appears in the entities list, DO NOT describe the object as moving independently. It moves with the person.\n",
      "3. Small handheld objects moving in the same direction as a person are almost certainly held items, not independent threats.\n",
      "4. Prioritize safety information about truly independent moving objects (vehicles, other people, animals).\n",
      "5. Ignore any coordinates or bounding box numbers (e.g., \"box (100, 200, ...)\") mentioned in the entities list. They are technical data. If an entity is described as \"at box\", simply treat it as \"present\" or \"in front of you\". Do NOT say \"at a box\", \"near a box\", or \"at the location\".\n",
      "--------------------\n",
      "ü§î Thinking...\n",
      "üó£Ô∏è A man is brushing his teeth.\n",
      "\n",
      "üì∏ Creating narration...\n",
      "   üìù BLIP Caption: a man is taking a picture of himself in the camera\n",
      "   üì§ Sending Prompt to Llama: \n",
      "--------------------\n",
      "SYSTEM: You are a helpful assistant for a blind user. Be concise and direct. Only describe what is certainly present. Do not ask questions.\n",
      "If the context mentions a \"mirror\" or \"reflection\" and it seems to be describing the user themselves (e.g., \"standing in front of a mirror\"), assume it is a hallucination caused by the camera feed and describe it as the person being present or facing the camera.\n",
      "\n",
      "USER:\n",
      "Context: \"a man is taking a picture of himself in the camera\"\n",
      "Entities (detected movement):\n",
      "- person at box (807, 483, 1493, 1073)\n",
      "\n",
      "TASK: Synthesize the context and entities into one natural sentence.\n",
      "IMPORTANT RULES:\n",
      "1. The entities likely correspond to the subjects in the context (e.g., a \"person\" entity is likely the same as \"a man\" or \"a woman\" in the context). Do not treat them as separate people unless clearly distinct.\n",
      "2. If the context mentions a person holding an object, and that object also appears in the entities list, DO NOT describe the object as moving independently. It moves with the person.\n",
      "3. Small handheld objects moving in the same direction as a person are almost certainly held items, not independent threats.\n",
      "4. Prioritize safety information about truly independent moving objects (vehicles, other people, animals).\n",
      "5. Ignore any coordinates or bounding box numbers (e.g., \"box (100, 200, ...)\") mentioned in the entities list. They are technical data. If an entity is described as \"at box\", simply treat it as \"present\" or \"in front of you\". Do NOT say \"at a box\", \"near a box\", or \"at the location\".\n",
      "--------------------\n",
      "ü§î Thinking...\n",
      "üó£Ô∏è There is a person present, facing the camera, taking a picture of themselves.\n",
      "\n",
      "üì∏ Creating narration...\n",
      "   üìù BLIP Caption: a man with headphones in his ears\n",
      "   üì§ Sending Prompt to Llama: \n",
      "--------------------\n",
      "SYSTEM: You are a helpful assistant for a blind user. Be concise and direct. Only describe what is certainly present. Do not ask questions.\n",
      "If the context mentions a \"mirror\" or \"reflection\" and it seems to be describing the user themselves (e.g., \"standing in front of a mirror\"), assume it is a hallucination caused by the camera feed and describe it as the person being present or facing the camera.\n",
      "\n",
      "USER:\n",
      "Context: \"a man with headphones in his ears\"\n",
      "Entities (detected movement):\n",
      "- person at box (579, 298, 1549, 1076)\n",
      "\n",
      "TASK: Synthesize the context and entities into one natural sentence.\n",
      "IMPORTANT RULES:\n",
      "1. The entities likely correspond to the subjects in the context (e.g., a \"person\" entity is likely the same as \"a man\" or \"a woman\" in the context). Do not treat them as separate people unless clearly distinct.\n",
      "2. If the context mentions a person holding an object, and that object also appears in the entities list, DO NOT describe the object as moving independently. It moves with the person.\n",
      "3. Small handheld objects moving in the same direction as a person are almost certainly held items, not independent threats.\n",
      "4. Prioritize safety information about truly independent moving objects (vehicles, other people, animals).\n",
      "5. Ignore any coordinates or bounding box numbers (e.g., \"box (100, 200, ...)\") mentioned in the entities list. They are technical data. If an entity is described as \"at box\", simply treat it as \"present\" or \"in front of you\". Do NOT say \"at a box\", \"near a box\", or \"at the location\".\n",
      "--------------------\n",
      "ü§î Thinking...\n",
      "üó£Ô∏è A man with headphones in his ears is present and facing you.\n",
      "‚ö†Ô∏è HAZARD: Warning: Person detected\n",
      "‚ö†Ô∏è HAZARD: Warning: Person detected\n",
      "üõë Stopped.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "print(\"Select Mode:\")\n",
    "print(\"1. Static Test Image\")\n",
    "print(\"2. Live Camera Demo\")\n",
    "\n",
    "choice = input(\"Enter choice (1 or 2): \")\n",
    "\n",
    "if choice == '2':\n",
    "    run_camera_demo()\n",
    "else:\n",
    "    run_image_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
