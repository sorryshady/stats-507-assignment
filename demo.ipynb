{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe My Environment - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the **Describe My Environment** system.\n",
    "\n",
    "**Features:**\n",
    "1. **Reflex Loop**: Low-latency object tracking and hazard detection (YOLO11).\n",
    "2. **Cognitive Loop**: Scene understanding (BLIP) and narration (Llama 3.2).\n",
    "3. **Audio Feedback**: Text-to-speech narration and hazard warnings.\n",
    "\n",
    "You can choose to run a **Static Test** (single image) or a **Live Camera Demo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.reflex_loop.tracker import YOLOTracker\n",
    "from src.reflex_loop.safety import SafetyMonitor\n",
    "from src.reflex_loop.physics import PhysicsEngine\n",
    "from src.cognitive_loop.scene_composer import SceneComposer\n",
    "from src.cognitive_loop.narrator import LLMNarrator\n",
    "from src.cognitive_loop.history import HistoryBuffer\n",
    "from src.hardware.audio import AudioHandler\n",
    "from src.config import GLOBAL_WARNING_COOLDOWN\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_image_test():\n",
    "    print(\"\\n--- Running Static Image Test ---\")\n",
    "    \n",
    "    # Initialize Components\n",
    "    print(\"â³ Initializing components...\")\n",
    "    tracker = YOLOTracker(model_path=\"yolo11n.pt\")\n",
    "    scene_composer = SceneComposer()\n",
    "    narrator = LLMNarrator()\n",
    "    audio_handler = AudioHandler()\n",
    "    \n",
    "    # Load Image\n",
    "    image_path = os.path.join(\"test_images\", \"test_image_0.jpg\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"âŒ Image not found: {image_path}\")\n",
    "        return\n",
    "        \n",
    "    frame = cv2.imread(image_path)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 1. Tracking\n",
    "    detections = tracker.detect(frame)\n",
    "    print(f\"âœ… Detected {len(detections)} objects\")\n",
    "    \n",
    "    # Visualize\n",
    "    annotated_frame = frame_rgb.copy()\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det.box\n",
    "        label = f\"{det.class_name} {det.confidence:.2f}\"\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated_frame)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Detections\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Scene Description\n",
    "    caption = scene_composer.generate_scene_description(frame)\n",
    "    print(f\"\\nðŸ“ Scene: {caption}\")\n",
    "    \n",
    "    # 3. Narration\n",
    "    # Prepare strings for compose_prompt\n",
    "    object_descriptions = [\n",
    "        f\"{d.class_name} (conf: {d.confidence:.2f})\" for d in detections\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸ¤– Generating Narration...\")\n",
    "    \n",
    "    # Use compose_prompt to create the string prompt for generate_narration\n",
    "    prompt = narrator.compose_prompt(caption, object_descriptions)\n",
    "    narration = narrator.generate_narration(prompt)\n",
    "    \n",
    "    if narration:\n",
    "        print(f\"\\nðŸ—£ï¸ FINAL NARRATION: {narration}\")\n",
    "        audio_handler.speak_text(narration)\n",
    "    else:\n",
    "        print(\"âš ï¸ Could not generate narration (check Ollama)\")\n",
    "        \n",
    "    # Allow time for audio to play before cleanup\n",
    "    time.sleep(5)\n",
    "    audio_handler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camera_demo():\n",
    "    print(\"\\n--- Running Live Camera Demo ---\")\n",
    "    print(\"â³ Initializing pipeline components...\")\n",
    "    \n",
    "    tracker = YOLOTracker(model_path=\"yolo11n.pt\")\n",
    "    scene_composer = SceneComposer()\n",
    "    narrator = LLMNarrator()\n",
    "    safety_monitor = SafetyMonitor()\n",
    "    history_buffer = HistoryBuffer()\n",
    "    audio_handler = AudioHandler()\n",
    "    physics_engine = PhysicsEngine()\n",
    "    \n",
    "    if not narrator.check_connection():\n",
    "        print(\"âš ï¸ Ollama is not running. Narration will be disabled.\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Could not open camera.\")\n",
    "        return\n",
    "        \n",
    "    # Get frame dimensions\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(\"âœ… Camera started.\")\n",
    "    print(\"Commands:\")\n",
    "    print(\" [Space] - Generate scene narration\")\n",
    "    print(\" [q]     - Quit\")\n",
    "\n",
    "    frame_count = 0\n",
    "    last_hazard_time = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            timestamp = time.time()\n",
    "            frame_count += 1\n",
    "            \n",
    "            # 1. Reflex Loop: Tracking\n",
    "            detections, annotated_frame = tracker.track(frame, frame_count, timestamp, return_annotated=True)\n",
    "            \n",
    "            # Update History\n",
    "            for det in detections:\n",
    "                track_id = det.track_id if det.track_id is not None else -1\n",
    "                history_buffer.add_detection(track_id, det)\n",
    "            \n",
    "            history_buffer.cleanup_stale_objects(frame_count)\n",
    "            \n",
    "            # 2. Hazard Detection\n",
    "            hazards = safety_monitor.check_hazards(detections, history_buffer)\n",
    "            if safety_monitor.should_warn(hazards):\n",
    "                current_time = time.time()\n",
    "                # Add cooldown to prevent spamming the audio queue\n",
    "                if current_time - last_hazard_time > GLOBAL_WARNING_COOLDOWN:\n",
    "                    warning = safety_monitor.get_warning_message(hazards)\n",
    "                    print(f\"âš ï¸ HAZARD: {warning}\")\n",
    "                    audio_handler.play_beep()\n",
    "                    audio_handler.speak_text(warning, priority=True)\n",
    "                    last_hazard_time = current_time\n",
    "                \n",
    "                # Visual warning always shows\n",
    "                cv2.putText(annotated_frame, \"HAZARD DETECTED\", (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "            # Show Frame\n",
    "            cv2.imshow(\"Describe My Environment - Demo\", annotated_frame)\n",
    "            \n",
    "            # Input Handling\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == 32:  # Spacebar for narration\n",
    "                print(\"\\nðŸ“¸ Creating narration...\")\n",
    "                \n",
    "                # Run narration logic\n",
    "                caption = scene_composer.generate_scene_description(frame)\n",
    "                print(f\"   ðŸ“ BLIP Caption: {caption}\")\n",
    "                \n",
    "                # Format objects for the prompt with motion data\n",
    "                # IMPORTANT: We do NOT use 'box coordinates' in the text sent to LLM to avoid hallucinations\n",
    "                object_descriptions = []\n",
    "                for d in detections:\n",
    "                    obj_desc = f\"{d.class_name}\"\n",
    "                    \n",
    "                    # Add motion context if tracked\n",
    "                    motion_tag = \"(stationary)\"\n",
    "                    if d.track_id is not None:\n",
    "                        tracked_obj = history_buffer.get_object(d.track_id)\n",
    "                        if tracked_obj:\n",
    "                            # Check if approaching center\n",
    "                            is_approaching = physics_engine.is_approaching_center(tracked_obj, frame_width, frame_height)\n",
    "                            # Check if expanding (getting closer)\n",
    "                            growth = physics_engine.calculate_area_growth(tracked_obj)\n",
    "                            \n",
    "                            if is_approaching and growth > 10.0:\n",
    "                                motion_tag = \"(approaching you)\"\n",
    "                            elif growth < -10.0:\n",
    "                                motion_tag = \"(moving away)\"\n",
    "                            else:\n",
    "                                # Calculate simple left/right movement\n",
    "                                vel_x, _ = physics_engine.calculate_velocity(tracked_obj)\n",
    "                                if vel_x > 2.0:\n",
    "                                    motion_tag = \"(moving right)\"\n",
    "                                elif vel_x < -2.0:\n",
    "                                    motion_tag = \"(moving left)\"\n",
    "                    \n",
    "                    obj_desc += f\" {motion_tag}\"\n",
    "                    object_descriptions.append(obj_desc)\n",
    "\n",
    "                if hazards:\n",
    "                    hazard_descriptions = [str(h.reason) for h in hazards]\n",
    "                    object_descriptions.append(f\"Hazards detected: {hazard_descriptions}\")\n",
    "                \n",
    "                # Compose the single string prompt\n",
    "                prompt = narrator.compose_prompt(caption, object_descriptions)\n",
    "                print(f\"   ðŸ“¤ Sending Prompt to Llama: \\n{'-'*20}\\n{prompt}\\n{'-'*20}\")\n",
    "                \n",
    "                # Generate narration\n",
    "                print(\"ðŸ¤” Thinking...\")\n",
    "                narration = narrator.generate_narration(prompt)\n",
    "                \n",
    "                if narration:\n",
    "                    print(f\"ðŸ—£ï¸ {narration}\")\n",
    "                    audio_handler.speak_text(narration)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        audio_handler.stop()\n",
    "        print(\"ðŸ›‘ Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Mode:\n",
      "1. Static Test Image\n",
      "2. Live Camera Demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Live Camera Demo ---\n",
      "â³ Initializing pipeline components...\n",
      "âœ… Camera started.\n",
      "Commands:\n",
      " [Space] - Generate scene narration\n",
      " [q]     - Quit\n",
      "\n",
      "ðŸ“¸ Creating narration...\n",
      "   ðŸ“ BLIP Caption: a man standing facing the camera\n",
      "   ðŸ“¤ Sending Prompt to Llama: \n",
      "--------------------\n",
      "SYSTEM: You are a helpful assistant for a blind user. Be concise and direct. Only describe what is certainly present. Do not ask questions.\n",
      "If the context mentions a \"mirror\" or \"reflection\" and it seems to be describing the user themselves (e.g., \"standing in front of a mirror\"), assume it is a hallucination caused by the camera feed and describe it as the person being present or facing the camera.\n",
      "\n",
      "USER:\n",
      "Context: \"a man standing facing the camera\"\n",
      "Entities (detected movement):\n",
      "- person (moving away)\n",
      "\n",
      "TASK: Synthesize the context and entities into one natural sentence.\n",
      "IMPORTANT RULES:\n",
      "1. MERGE SUBJECTS: The \"Entities\" list (e.g., \"person\") usually refers to the SAME subject mentioned in the \"Context\" (e.g., \"man\", \"woman\"). Assume they are the SAME person unless the context explicitly describes multiple distinct people (e.g., \"two men\", \"a crowd\").\n",
      "2. If the context mentions a person holding an object, and that object also appears in the entities list, DO NOT describe the object as moving independently. It moves with the person.\n",
      "3. Small handheld objects moving in the same direction as a person are almost certainly held items, not independent threats.\n",
      "4. Prioritize safety information about truly independent moving objects (vehicles, other people, animals).\n",
      "5. Ignore any coordinates or bounding box numbers (e.g., \"box (100, 200, ...)\") mentioned in the entities list. They are technical data. If an entity is described as \"at box\", simply treat it as \"present\" or \"in front of you\". Do NOT say \"at a box\", \"near a box\", or \"at the location\".\n",
      "--------------------\n",
      "ðŸ¤” Thinking...\n",
      "ðŸ—£ï¸ The person is moving away from the camera.\n",
      "âš ï¸ HAZARD: Warning: Person detected\n",
      "ðŸ›‘ Stopped.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "print(\"Select Mode:\")\n",
    "print(\"1. Static Test Image\")\n",
    "print(\"2. Live Camera Demo\")\n",
    "\n",
    "choice = input(\"Enter choice (1 or 2): \")\n",
    "\n",
    "if choice == '2':\n",
    "    run_camera_demo()\n",
    "else:\n",
    "    run_image_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
