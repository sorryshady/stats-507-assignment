{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe My Environment - Core Functionality Demo\n",
    "\n",
    "This notebook demonstrates the core machine learning pipeline of the **Describe My Environment** project.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Input:** Load a test image\n",
    "2. **Reflex Loop:** Detect and track objects (YOLO11)\n",
    "3. **Cognitive Loop:** Generate scene caption (BLIP) and narration (Llama 3.2)\n",
    "4. **Output:** Visualize results and narration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.reflex_loop.tracker import YOLOTracker\n",
    "from src.cognitive_loop.scene_composer import SceneComposer\n",
    "from src.cognitive_loop.narrator import LLMNarrator\n",
    "from src.config import NARRATION_CONFIG\n",
    "\n",
    "print(\"\u2705 Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "image_path = os.path.join(\"test_images\", \"test_image_0.jpg\")\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"\u274c Image not found: {image_path}\")\n",
    "else:\n",
    "    # Read with OpenCV (BGR)\n",
    "    frame = cv2.imread(image_path)\n",
    "    # Convert to RGB for display\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reflex Loop: Object Detection\n",
    "We use **YOLO11n** to detect objects in the scene. This simulates the \"Reflex Loop\" which provides low-latency awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tracker\n",
    "print(\"\u23f3 Initializing YOLO tracker...\")\n",
    "tracker = YOLOTracker(model_path=\"yolo11n.pt\")\n",
    "\n",
    "# Run detection\n",
    "detections = tracker.process_frame(frame)\n",
    "print(f\"\u2705 Detected {len(detections)} objects\")\n",
    "\n",
    "# Visualize detections\n",
    "annotated_frame = frame_rgb.copy()\n",
    "\n",
    "for det in detections:\n",
    "    box = det['box']\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    label = f\"{det['class_name']} {det['confidence']:.2f}\"\n",
    "    \n",
    "    # Draw box\n",
    "    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    # Draw label\n",
    "    cv2.putText(annotated_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(annotated_frame)\n",
    "plt.axis('off')\n",
    "plt.title(\"Reflex Loop: Detections\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cognitive Loop: Scene Understanding\n",
    "We use **BLIP** to generate a descriptive caption of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scene Composer (BLIP)\n",
    "print(\"\u23f3 Initializing Scene Composer (BLIP)...\")\n",
    "scene_composer = SceneComposer()\n",
    "\n",
    "# Generate caption\n",
    "caption = scene_composer.describe_scene(frame)\n",
    "print(f\"\\n\ud83d\udcdd BLIP Caption: \\\"{caption}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cognitive Loop: AI Narration\n",
    "We use **Llama 3.2 (via Ollama)** to synthesize the detection data and scene caption into a helpful narration for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Narrator\n",
    "print(\"\u23f3 Initializing Narrator (Ollama)...\")\n",
    "narrator = LLMNarrator()\n",
    "\n",
    "# Check if Ollama is running\n",
    "if not narrator.check_connection():\n",
    "    print(\"\u26a0\ufe0f  Ollama is not running. Please start it with 'ollama serve' to see narration.\")\n",
    "else:\n",
    "    # Prepare context\n",
    "    context = {\n",
    "        \"scene_description\": caption,\n",
    "        \"objects\": [\n",
    "            {\"class_name\": d['class_name'], \"confidence\": d['confidence'], \"box\": d['box']} \n",
    "            for d in detections\n",
    "        ],\n",
    "        \"hazards\": [] # No hazards for this static image test\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\ud83e\udd16 Generating Narration...\")\n",
    "    narration = narrator.generate_narration(context)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"\ud83d\udcac FINAL NARRATION:\")\n",
    "    print(\"=\"*50)\n",
    "    print(narration)\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}