# Product Context

## Problem Statement
Visually impaired individuals often rely on tools that provide basic object recognition (e.g., "chair", "table"). These tools lack **situational awareness**â€”they don't understand movement, direction, or context (e.g., "a cyclist approaching fast from the right").

## Solution: "Describe My Environment"
A dual-loop AI system that balances safety and understanding:
- **Reflexes:** Immediate protection. "STOP! Hazard ahead."
- **Cognition:** Rich understanding. "You are in a park. A dog is playing fetch to your left."

## User Experience Goals
1.  **Safety First:** Latency-critical hazards take priority over everything.
2.  **Natural Interaction:** The system should feel like a helpful sighted companion, not a robot reading a list.
3.  **On-Demand Detail:** Users shouldn't be overwhelmed. Detailed narration is triggered only when asked.
4.  **Web Accessibility:** The web interface is a testing ground/demo for the wearable concept, accessible via browser.

## Current Focus
- Transforming the initial CLI/script-based prototype into a robust Web Application.
- Ensuring the web interface accurately reflects the "wearable" experience (camera feed, audio output).

