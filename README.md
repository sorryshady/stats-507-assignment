# Describe My Environment: Project Master Plan (v2.0)

**Course:** STATS 507  
**Focus:** Full-Stack ML Engineering / Edge Computing  
**Target Hardware:** Apple Silicon (M4 Pro) & iPhone 17 Pro (Ultra-Wide)

---

## 1. ğŸ¯ Core Mission

To build a **wearable-simulated AI assistant** for visually impaired users that creates a comprehensive understanding of the physical world. Unlike traditional tools that simply list objects ("Person, Chair"), this system provides **spatial and temporal context** ("A person is walking towards you from the left").

### The "Dual-Loop" Philosophy

The human brain processes vision in two modes: **Reflexes** (fast, survival-based) and **Cognition** (slow, detail-based). This project mimics that biology:

1.  **The Reflex Loop (Safety):** Runs at 30 FPS. Instantly warns of physical collision risks.
2.  **The Cognitive Loop (Narrator):** Runs On-Demand. Synthesizes history and vision into natural language stories.

---

## 2. ğŸ—ï¸ Architectural Blueprint

### High-Level Data Flow

```mermaid
graph TD
    %% Hardware Layer
    Input["iPhone 17 Pro Camera (0.5x Ultra-Wide)"] -->|Raw Frame Stream| AppEntryPoint

    %% The Dual-Loop Split
    AppEntryPoint -->|Stream Copy| SafetyLoop
    AppEntryPoint -->|Snapshot| CognitiveLoop

    %% 1. The Reflex Loop (Fast)
    subgraph "Reflex Loop (30 FPS)"
        SafetyLoop[YOLO11 Tracking]
        SafetyLoop -->|BBox Data| PhysicsEngine["Physics Engine (Velocity/Expansion Calc)"]
        PhysicsEngine -->|Hazard Detected?| AudioReflex["Audio Beep / 'STOP'"]
    end

    %% 2. The Cognitive Loop (Slow/Smart)
    subgraph "Cognitive Loop (On-Demand)"
        CognitiveLoop[Input Handler]
        CognitiveLoop -->|Current Frame| BLIP["BLIP Model (Scene Context)"]
        CognitiveLoop -->|Last 90 Frames| History["History Buffer (Trajectory Analysis)"]

        BLIP -->|Context String| Fusion
        History -->|Movement String| Fusion

        Fusion["Llama 3.2 3B (Narrator Agent)"] -->|Natural Language| TTS[Text-to-Speech]
    end

    %% Outputs
    AudioReflex --> User((User))
    TTS --> User
```

---

## 3. ğŸ› ï¸ The Tech Stack (v2.0)

We have upgraded the stack to leverage the M4 Pro's neural engine and the iPhone's advanced optics.

| Component         | Technology             | Role               | Reason for Choice                                                              |
| :---------------- | :--------------------- | :----------------- | :----------------------------------------------------------------------------- |
| **Input**         | **iPhone 17 Pro**      | Vision Sensor      | The **48MP Ultra-Wide (0.5x)** lens eliminates blind spots crucial for safety. |
| **Tracking**      | **YOLO11n-track**      | Object Persistence | Tracks unique IDs to calculate trajectory (Velocity & Direction).              |
| **Vision**        | **BLIP**               | Scene Captioning   | Provides the "Gist" of the scene (e.g., "A messy bedroom").                    |
| **Reasoning**     | **Llama 3.2 (3B)**     | Data Fusion        | Converts raw JSON data into human-like, helpful narration.                     |
| **Inference**     | **Ollama (Metal)**     | LLM Runner         | Optimized for Mac M4 GPU/NPU; runs locally with zero latency penalty.          |
| **Orchestration** | **Python (Threading)** | Controller         | Manages the async relationship between the fast and slow loops.                |

---

## 4. ğŸ“ Project Structure

```
final/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ DEPLOYMENT.md             # Deployment guide (Local + Vercel)
â”œâ”€â”€ requirements.txt          # Root Python dependencies
â”œâ”€â”€ demo.ipynb                # Jupyter Notebook Demo (Core Pipeline)
â”œâ”€â”€ run.py                    # Legacy CLI entry point
â”œâ”€â”€ yolo11n.pt                # YOLO model weights
â”‚
â”œâ”€â”€ backend/                  # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API Routes & Models
â”‚   â”‚   â”œâ”€â”€ core/             # Core Logic
â”‚   â”‚   â”œâ”€â”€ middleware/       # CORS & Middleware
â”‚   â”‚   â””â”€â”€ main.py           # App Entry Point
â”‚   â”œâ”€â”€ tests/                # Backend Tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/                 # Next.js Frontend
â”‚   â”œâ”€â”€ app/                  # Next.js App Router Pages
â”‚   â”œâ”€â”€ components/           # React Components
â”‚   â”œâ”€â”€ hooks/                # Custom React Hooks
â”‚   â”œâ”€â”€ lib/                  # Utilities & Types
â”‚   â”œâ”€â”€ public/               # Static Assets
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/                      # Core ML codebase (Shared Logic)
â”‚   â”œâ”€â”€ main.py               # Dual-loop system orchestrator
â”‚   â”œâ”€â”€ config.py             # Configuration constants
â”‚   â”œâ”€â”€ hardware/             # Camera & audio handlers
â”‚   â”œâ”€â”€ reflex_loop/          # Safety monitoring (30 FPS)
â”‚   â”œâ”€â”€ cognitive_loop/       # Scene narration (on-demand)
â”‚   â””â”€â”€ utils/                # Data structures & threading
â”‚
â”œâ”€â”€ report/                   # Final Report
â”‚   â””â”€â”€ README.md             # Report placeholder/link
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md             # Documentation index
â”‚   â”œâ”€â”€ USAGE.md              # Detailed usage guide
â”‚   â”œâ”€â”€ CONTROL_FLOW.md       # System architecture diagrams
â”‚   â”œâ”€â”€ logging.md            # Logging strategy
â”‚   â”œâ”€â”€ WEB_APP_PLAN.md       # Web app architecture plan
â”‚   â”œâ”€â”€ CHALLENGES_AND_LIMITATIONS.md
â”‚   â”œâ”€â”€ WEARABLE_ARCHITECTURE.md
â”‚   â””â”€â”€ future_expansion_plan.md
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ verify_ollama.py
â”‚   â”œâ”€â”€ verify_tts.py
â”‚   â””â”€â”€ list_cameras.py
â”‚
â”œâ”€â”€ tests/                    # Core System Unit Tests
â”‚   â”œâ”€â”€ test_physics.py
â”‚   â”œâ”€â”€ test_safety.py
â”‚   â””â”€â”€ test_trajectory.py
â”‚
â””â”€â”€ test_images/              # Test image dataset
```

---

## 5. ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/sorryshady/stats-507-assignment.git
cd final

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify Ollama setup (required for Narration)
python scripts/verify_ollama.py
```

### 2. Run the Jupyter Demo

This demonstrates the core ML pipeline without web servers.

```bash
jupyter notebook demo.ipynb
```

### 3. Run the Full Web Application

**Backend:**

```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Frontend:**

```bash
# In a new terminal
cd frontend
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

### 4. Run the CLI Application (Legacy)

```bash
python run.py
```

---

## 6. ğŸ“š Documentation

- **[Usage Guide](docs/USAGE.md)** - Detailed running instructions
- **[System Architecture](docs/CONTROL_FLOW.md)** - Detailed control flow
- **[Logging System](docs/logging.md)** - Logging documentation
- **[Web App Plan](docs/WEB_APP_PLAN.md)** - Full-stack architecture

## 7. ğŸ—“ï¸ Current Status

**Status:** âœ… Completed (Dec 3, 2025)

- âœ… Core ML pipeline (YOLO, BLIP, Llama 3.2)
- âœ… CLI application with dual-loop system
- âœ… Backend API (FastAPI)
- âœ… Frontend (Next.js)
- âœ… Jupyter notebook demo
- âœ… Final Report (in `report/`)
