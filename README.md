# Describe My Environment: Project Master Plan (v2.0)

**Course:** STATS 507  
**Focus:** Full-Stack ML Engineering / Edge Computing  
**Target Hardware:** Apple Silicon (M4 Pro) with camera input support (webcam or iPhone Continuity Camera)

ğŸŒ **Project Website:** [https://stats-507-assignment.vercel.app/](https://stats-507-assignment.vercel.app/)

---

## 1. Core Mission

To build a **wearable-simulated AI assistant** for visually impaired users that creates a comprehensive understanding of the physical world. Unlike traditional tools that simply list objects ("Person, Chair"), this system provides **spatial and temporal context** ("A person is walking towards you from the left").

### The "Dual-Loop" Philosophy

The human brain processes vision in two modes: **Reflexes** (fast, survival-based) and **Cognition** (slow, detail-based). This project mimics that biology:

1.  **The Reflex Loop (Safety):** Runs at 30 FPS. Instantly warns of physical collision risks.
2.  **The Cognitive Loop (Narrator):** Runs On-Demand. Synthesizes history and vision into natural language stories.

---

## 2. Architectural Blueprint

### High-Level Data Flow

```mermaid
graph TD
    %% Hardware Layer
    Input["camera input"] -->|Raw Frame Stream| AppEntryPoint

    %% The Dual-Loop Split
    AppEntryPoint -->|Stream Copy| SafetyLoop
    AppEntryPoint -->|Snapshot| CognitiveLoop

    %% 1. The Reflex Loop (Fast)
    subgraph "Reflex Loop (30 FPS)"
        SafetyLoop[YOLO11 Tracking]
        SafetyLoop -->|BBox Data| PhysicsEngine["Physics Engine (Velocity/Expansion Calc)"]
        PhysicsEngine -->|Hazard Detected?| AudioReflex["Audio Beep / 'STOP'"]
    end

    %% 2. The Cognitive Loop (Slow/Smart)
    subgraph "Cognitive Loop (OnDemand)"
        CognitiveLoop[Input Handler]
        CognitiveLoop -->|Current Frame| BLIP["BLIP Model (Scene Context)"]
        CognitiveLoop -->|Last 90 Frames| History["History Buffer (Trajectory Analysis)"]

        BLIP -->|Context String| Fusion
        History -->|Movement String| Fusion

        Fusion["Llama 3.2 3B (Narrator Agent)"] -->|Natural Language| TTS[Text-to-Speech]
    end

    %% Outputs
    AudioReflex --> User((User))
    TTS --> User
```

---

## 3. The Tech Stack (v2.0)

The stack leverages the M4 Pro's neural engine and the iPhone's advanced optics.

| Component         | Technology             | Role               | Reason for Choice                                                              |
| :---------------- | :--------------------- | :----------------- | :----------------------------------------------------------------------------- |
| **Input**         | **iPhone 17 Pro**      | Vision Sensor      | The **48MP Ultra-Wide (0.5x)** lens eliminates blind spots crucial for safety. |
| **Tracking**      | **YOLO11n-track**      | Object Persistence | Tracks unique IDs to calculate trajectory (Velocity & Direction).              |
| **Vision**        | **BLIP**               | Scene Captioning   | Provides the "Gist" of the scene (e.g., "A messy bedroom").                    |
| **Reasoning**     | **Llama 3.2 (3B)**     | Data Fusion        | Converts raw JSON data into human-like, helpful narration.                     |
| **Inference**     | **Ollama (Metal)**     | LLM Runner         | Optimized for Mac M4 GPU/NPU; runs locally with zero latency penalty.          |
| **Orchestration** | **Python (Threading)** | Controller         | Manages the async relationship between the fast and slow loops.                |

---

## 4. Project Structure

```
final/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ DEPLOYMENT.md             # Deployment guide (Local + Vercel)
â”œâ”€â”€ requirements.txt          # Root Python dependencies
â”œâ”€â”€ demo.ipynb                # Jupyter Notebook Demo (Core Pipeline)
â”œâ”€â”€ run.py                    # Legacy CLI entry point
â”œâ”€â”€ yolo11n.pt                # YOLO model weights
â”‚
â”œâ”€â”€ backend/                  # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API Routes & Models
â”‚   â”‚   â”œâ”€â”€ core/             # Core Logic
â”‚   â”‚   â”œâ”€â”€ middleware/       # CORS & Middleware
â”‚   â”‚   â””â”€â”€ main.py           # App Entry Point
â”‚   â”œâ”€â”€ tests/                # Backend Tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/                 # Next.js Frontend
â”‚   â”œâ”€â”€ app/                  # Next.js App Router Pages
â”‚   â”œâ”€â”€ components/           # React Components
â”‚   â”œâ”€â”€ hooks/                # Custom React Hooks
â”‚   â”œâ”€â”€ lib/                  # Utilities & Types
â”‚   â”œâ”€â”€ public/               # Static Assets
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/                      # Core ML codebase (Shared Logic)
â”‚   â”œâ”€â”€ main.py               # Dual-loop system orchestrator
â”‚   â”œâ”€â”€ config.py             # Configuration constants
â”‚   â”œâ”€â”€ hardware/             # Camera & audio handlers
â”‚   â”œâ”€â”€ reflex_loop/          # Safety monitoring (30 FPS)
â”‚   â”œâ”€â”€ cognitive_loop/       # Scene narration (on-demand)
â”‚   â””â”€â”€ utils/                # Data structures & threading
â”‚
â”œâ”€â”€ report/                   # Final Report
â”‚   â””â”€â”€ README.md             # Report placeholder/link
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md             # Documentation index
â”‚   â”œâ”€â”€ USAGE.md              # Detailed usage guide
â”‚   â”œâ”€â”€ CONTROL_FLOW.md       # System architecture diagrams
â”‚   â”œâ”€â”€ logging.md            # Logging strategy
â”‚   â”œâ”€â”€ WEB_APP_PLAN.md       # Web app architecture plan
â”‚   â”œâ”€â”€ CHALLENGES_AND_LIMITATIONS.md
â”‚   â”œâ”€â”€ WEARABLE_ARCHITECTURE.md
â”‚   â””â”€â”€ future_expansion_plan.md
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ verify_ollama.py
â”‚   â”œâ”€â”€ verify_tts.py
â”‚   â””â”€â”€ list_cameras.py
â”‚
â”œâ”€â”€ tests/                    # Core System Unit Tests
â”‚   â”œâ”€â”€ test_physics.py
â”‚   â”œâ”€â”€ test_safety.py
â”‚   â””â”€â”€ test_trajectory.py
â”‚
â””â”€â”€ test_images/              # Test image dataset
```

---

## 5. Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/sorryshady/stats-507-assignment.git
cd final

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify Ollama setup (required for Narration)
python scripts/verify_ollama.py
```

### 2. Run the Jupyter Demo

This demonstrates the core ML pipeline without web servers.

```bash
jupyter notebook demo.ipynb
```

### 3. Run the Full Web Application

**Backend:**

```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Frontend:**

```bash
# In a new terminal
cd frontend
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

### 4. Run the CLI Application (Legacy)

```bash
python run.py
```

---

## 6. Live Demo & Project Website

ğŸŒ **Deployed Website:** [https://stats-507-assignment.vercel.app/](https://stats-507-assignment.vercel.app/)

The deployed website showcases:

- Project overview and architecture
- Demo videos demonstrating the system's capabilities
- Technical documentation and features
- About page with project details

**Note:** The deployed website displays demo videos and project information. For **interactive functionality** (real-time camera feed, object detection, and narration), you need to **clone the repository and run it locally** following the [Quick Start](#5-quick-start) guide above.

---

## 7. Documentation

- **[Usage Guide](docs/USAGE.md)** - Detailed running instructions
- **[System Architecture](docs/CONTROL_FLOW.md)** - Detailed control flow
- **[Web App Plan](docs/WEB_APP_PLAN.md)** - Full-stack architecture

---

## 8. Current Status

**Status:** Completed (Dec 3, 2025)

- Core ML pipeline (YOLO, BLIP, Llama 3.2)
- CLI application with dual-loop system
- Backend API (FastAPI)
- Frontend (Next.js)
- Jupyter notebook demo
- Final Report (in `report/`)
